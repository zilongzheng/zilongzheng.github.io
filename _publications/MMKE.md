---
layout: pub
type: article
key: iclr25mmke
title: >
    MMKE-Bench: A Multimodal Editing Benchmark for Diverse Visual Knowledge
author: Du, Yuntao and Jiang, Kailin and Gao, Zhi and Shi, Chenrui and Zheng, Zilong and Qi, Siyuan and Li, Qing
# abbr: EMNLP'24
correspondence: Zheng, Zilong and Li, Qing
# pdf: https://aclanthology.org/2023.emnlp-main.334.pdf
abbr: ICLR'25
# journal: The Tenth International Conference on Learning Representations (ICLR)
journal: ICLR
pdf: https://openreview.net/forum?id=v8qABSeeKO
year: 2025
selected: false
abstract: >
    Knowledge editing techniques have emerged as essential tools for updating the factual knowledge of large language models (LLMs) and multimodal models (LMMs), allowing them to correct outdated or inaccurate information without retraining from scratch. However, existing benchmarks for multimodal knowledge editing primarily focus on entity-level knowledge represented as simple triplets, which fail to capture the complexity of real-world multimodal information. To address this issue, we introduce MMKE-Bench, a comprehensive MultiModal Knowledge Editing Benchmark, designed to evaluate the ability of LMMs to edit diverse visual knowledge in real-world scenarios. MMKE-Bench addresses these limitations by incorporating three types of editing tasks: visual entity editing, visual semantic editing, and user-specific editing. Besides, MMKE-Bench uses free-form natural language to represent and edit knowledge, offering a more flexible and effective format. The benchmark consists of 2,940 pieces of knowledge and 7,229 images across 110 fine-grained types, with evaluation questions automatically generated and human-verified. We assess five state-of-the-art knowledge editing methods on three prominent LMMs, revealing that no method excels across all criteria, and that visual and user-specific edits are particularly challenging. MMKE-Bench sets a new standard for evaluating the robustness of multimodal knowledge editing techniques, driving progress in this rapidly evolving field.
bibtex: >
    @inproceedings{du2025mmke,
        title={MMKE-Bench: A Multimodal Editing Benchmark for Diverse Visual Knowledge}, 
        author={Du, Yuntao and Jiang, Kailin and Gao, Zhi and Shi, Chenrui and Zheng, Zilong and Qi, Siyuan and Li, Qing},
        booktitle={The Thirteenth International Conference on Learning Representations},
        year={2025}
    }
---