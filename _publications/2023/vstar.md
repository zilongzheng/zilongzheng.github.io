---
layout: publication
type: inproceedings
title: >
    VSTAR: A Video-grounded Dialogue Dataset for Situated Semantic Understanding with Scene and Topic Transitions
author: Wang, Yuxuan and Zheng#, Zilong and Zhao, Xueliang and Li, Jinpeng and Wang, Yueqian, and Zhao#, Dongyan
abbr: ACL'23
# booktitle: Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL)
booktitle: ACL
year: 2023
arxiv: 2305.18756
pdf: https://aclanthology.org/2023.acl-long.276/
code: https://github.com/patrick-tssn/VSTAR
selected: false
website: https://vstar-benchmark.github.io
abstract: >
    Video-grounded dialogue understanding is a challenging problem that requires machine to perceive, parse and reason over situated semantics extracted from weakly aligned video and dialogues. Most existing benchmarks treat both modalities the same as a frame-independent visual understanding task, while neglecting the intrinsic attributes in multimodal dialogues, such as scene and topic transitions. In this paper, we present Video-grounded Scene&Topic AwaRe dialogue (VSTAR) dataset, a large scale video-grounded dialogue understanding dataset based on 395 TV series. Based on VSTAR, we propose two benchmarks for video-grounded dialogue understanding: scene segmentation and topic segmentation, and one benchmark for video-grounded dialogue generation. Comprehensive experiments are performed on these benchmarks to demonstrate the importance of multimodal information and segments in video-grounded dialogue understanding and generation.
bibtex: >
    @inproceedings{wang2023vstar,
        title={VSTAR: A Video-grounded Dialogue Dataset for Situated Semantic Understanding with Scene and Topic Transitions},
        author={Wang, Yuxuan and Zheng, Zilong and Zhao, Xueliang and Li, Jinpeng and Wang, Yueqian, and Zhao, Dongyan},
        booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL)},
        year={2023}
    }
---
