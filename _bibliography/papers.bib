---
---

@string{cvpr = {IEEE Conference on Computer Vision and Pattern Recognition}}
@string{icml = {International Conference on Machine Learning}}
@string{acl = {Annual Meeting of the Association for Computational Linguistics}}
@string{iclr = {International Conference on Learning Representations}}
@string{aaai = {AAAI Conference on Artificial Intelligence}}
@string{cogsci = {Annual Meeting of the Cognitive Science Society}}
@string{iccv = {IEEE International Conference on Computer Vision}}
@string{arxiv = {arXiv preprint,}}

@article{zhao2025absolute,
  bibtex_show={true},
  title={Absolute Zero: Reinforced Self-play Reasoning with Zero Data},
  author={Zhao*, Andrew and Wu*, Yiran and Yue, Yang and Wu, Tong and Xu, Quentin and Yue, Yang and Lin, Matthieu and Wang, Shenzhi and Wu, Qingyun and Zheng#, Zilong and Huang#, Gao},
  abstract={Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning capabilities of large language models by learning directly from outcome-based rewards. Recent RLVR works that operate under the zero setting avoid supervision in labeling the reasoning process, but still depend on manually curated collections of questions and answers for training. The scarcity of high-quality, human-produced examples raises concerns about the long-term scalability of relying on human supervision, a challenge already evident in the domain of language model pretraining. Furthermore, in a hypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited learning potential for a superintelligent system. To address these concerns, we propose a new RLVR paradigm called Absolute Zero, in which a single model learns to propose tasks that maximize its own learning progress and improves reasoning by solving them, without relying on any external data. Under this paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training curriculum and reasoning ability by using a code executor to both validate proposed code reasoning tasks and verify answers, serving as an unified source of verifiable reward to guide open-ended yet grounded learning. Despite being trained entirely without external data, AZR achieves overall SOTA performance on coding and mathematical reasoning tasks, outperforming existing zero-setting models that rely on tens of thousands of in-domain human-curated examples. Furthermore, we demonstrate that AZR can be effectively applied across different model scales and is compatible with various model classes.},
  journal={Preprint},
  arxiv={2505.03335},
  selected={true},
  website={https://andrewzh112.github.io/absolute-zero-reasoner/},
  year={2025},
  code={https://github.com/LeapLabTHU/Absolute-Zero-Reasoner},
  github_star={true},
  model={https://huggingface.co/collections/andrewzh/absolute-zero-reasoner-68139b2bca82afb00bc69e5b},
  medias={X:https://x.com/_AndrewZhao/status/1919920459748909288;HF Papers: https://huggingface.co/papers/2505.03335},
}

@inproceedings{zhu2025toedit,
      bibtex_show={true},
      abbr={ICML'25},
      title={How to Synthesize Text Data without Model Collapse?},
      author={Xuekai Zhu and Daixuan Cheng and Hengli Li and Kaiyan Zhang and Ermo Hua and Xingtai Lv and Ning Ding and Zhouhan Lin# and Zilong Zheng# and Bowen Zhou#},
      year={2025},
      booktitle={ICML},
      arxiv={2412.14689},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.14689},
      selected={true},
      abstract={Model collapse in synthetic data indicates that iterative training on self-generated data leads to a gradual decline in performance. With the proliferation of AI models, synthetic data will fundamentally reshape the web data ecosystem. Future GPT-{n} models will inevitably be trained on a blend of synthetic and human-produced data. In this paper, we focus on two questions: what is the impact of synthetic data on language model training, and how to synthesize data without model collapse? We first pre-train language models across different proportions of synthetic data, revealing a negative correlation between the proportion of synthetic data and model performance. We further conduct statistical analysis on synthetic data to uncover distributional shift phenomenon and over-concentration of n-gram features. Inspired by the above findings, we propose token editing on human-produced data to obtain semi-synthetic data. As a proof of concept, we theoretically demonstrate that token-level editing can prevent model collapse, as the test error is constrained by a finite upper bound. We conduct extensive experiments on pre-training from scratch, continual pre-training, and supervised fine-tuning. The results validate our theoretical proof that token-level editing improves data quality and enhances model performance.},
}

@inproceedings{zheng2025mcu,
      bibtex_show={true},
      title={MCU: An Evaluation Framework for Open-Ended Game Agents},
      author={Xinyue Zheng* and Haowei Lin* and Kaichen He and Zihao Wang and Zilong Zheng# and Yitao Liang#},
      year={2025},
      arxiv={2310.08367},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2310.08367},
      selected={true},
      booktitle={ICML},
      abbr={ICML'25},
      award={Spotlight Paper},
      abstract={Developing AI agents capable of interacting with open-world environments to solve diverse tasks is a compelling challenge. However, evaluating such open-ended agents remains difficult, with current benchmarks facing scalability limitations. To address this, we introduce <em>Minecraft Universe</em> (MCU), a comprehensive evaluation framework set within the open-world video game Minecraft. MCU incorporates three key components: (1) an expanding collection of 3,452 composable atomic tasks that encompasses 11 major categories and 41 subcategories of challenges; (2) a task composition mechanism capable of generating infinite diverse tasks with varying difficulty; and (3) a general evaluation framework that achieves 91.5% alignment with human ratings for open-ended task assessment. Empirical results reveal that even state-of-the-art foundation agents struggle with the increasing diversity and complexity of tasks. These findings highlight the necessity of MCU as a robust benchmark to drive progress in AI agent development within open-ended environments.},
      code={https://github.com/CraftJarvis/MCU},
}

@inproceedings{wu2025tokenswift,
    bibtex_show={true},
    abbr={ICML'25},
    title={Lossless Acceleration of Ultra Long Sequence Generation},
    author={Tong Wu and Junzhe Shen and Zixia Jia and Yuxuan Wang and Zilong Zheng#},
    year={2025},
    arxiv={2502.18890},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2502.18890},
    selected={true},
    booktitle={ICML},
    code={https://github.com/bigai-nlco/TokenSwift},
    github_star={true},
    medias={X:https://x.com/ZilongZheng/status/1896843753446609382},
    abstract={Generating ultra-long sequences with large language models (LLMs) has become increasingly crucial but remains a highly time-intensive task, particularly for sequences up to 100K tokens. While traditional speculative decoding methods exist, simply extending their generation limits fails to accelerate the process and can be detrimental. Through an in-depth analysis, we identify three major challenges hindering efficient generation: frequent model reloading, dynamic key-value (KV) management and repetitive generation. To address these issues, we introduce TOKENSWIFT, a novel framework designed to substantially accelerate the generation process of ultra-long sequences while maintaining the target model's inherent quality. Experimental results demonstrate that TOKENSWIFT achieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B, 14B) and architectures (MHA, GQA). This acceleration translates to hours of time savings for ultra-long sequence generation, establishing TOKENSWIFT as a scalable and effective solution at unprecedented lengths. Code can be found at <a href="https://github.com/bigai-nlco/TokenSwift">this URL</a>.}
}

@inproceedings{lin2025bidirection,
    bibtex_show={true},
    abbr={ACL'25},
    title={Look Both Ways and No Sink: Converting LLMs into Text Encoders without Training},
    author={Lin*, Ziyong and Wu*, Haoyi and Wang, Shu and Tu#, Kewei and Zheng#, Zilong and Jia#, Zixia},
    booktitle={ACL},
    year={2025},
    abstract={Recent advancements have demonstrated the advantage of converting pretrained large language models into powerful text encoders by enabling bidirectional attention in transformer layers. However, existing methods often require extensive training on large-scale datasets, posing challenges in domain-specific scenarios. In this work, we show that a domain-specific pretrained large language model can be converted into a strong domain-specific text encoder without additional training. We first conduct a comprehensive empirical study to investigate different conversion strategies and identify the impact of the attention sink phenomenon on the performance of converted encoder models. Based on our findings, we propose a novel approach that enables bidirectional attention and suppresses the attention sink phenomenon, resulting in superior performance. Extensive experiments on multiple domains demonstrate the effectiveness of our approach. Our work provides new insights into the training-free conversion of text encoders in low-resource scenarios and contributes to the advancement of domain-specific text representation generation.}
}

@inproceedings{lin2025bidirection,
    bibtex_show={true},
    abbr={CogSci'25},
    title={Probing and Inducing Combinational Creativity in Vision-Language Models},
    author={Peng*, Yongqian and Ma*, Yuxi and Wang, Mengmeng and Wang, Yuxuan and Wang, Yizhou and Zhang, Chi and Zhu#, Yixin and Zheng#, Zilong},
    booktitle={CogSci},
    year={2025},
    arxiv={2504.13120},
    data={https://drive.google.com/drive/folders/1XIvOVwP0eVX60L-STugt_vi19Q_kAEMW},
    video={https://vimeo.com/1075960292},
    code={https://github.com/PPYYQQ/aicc-code},
    award={Oral},
    abstract={The ability to combine existing concepts into novel ideas stands as a fundamental hallmark of human intelligence. Recent advances in VLMs like GPT-4V and DALLE-3 have sparked debate about whether their outputs reflect combinational creativity---defined by M. A. Boden(1998) as synthesizing novel ideas through combining existing concepts---or sophisticated pattern matching of training data. Drawing inspiration from cognitive science, we investigate the combinational creativity of VLMs from the lens of concept blending. We propose the Identification-Explanation-Implication(IEI) framework, which decomposes creative processes into three levels: identifying input spaces, extracting shared attributes, and deriving novel semantic implications. To validate this framework, we curate CreativeMashup, a high-quality dataset of 666 artist-generated visual mashups annotated according to the IEI framework. Through extensive experiments, we demonstrate that in comprehension tasks, best VLMs have surpassed average human performance while falling short of expert-level understanding; in generation tasks, incorporating our IEI framework into the generation pipeline significantly enhances the creative quality of VLMs outputs. Our findings establish both a theoretical foundation for evaluating artificial creativity and practical guidelines for improving creative generation in VLMs.}
}

@article{zhao2025diverct,
    title={DiveR-CT: Diversity-enhanced Red Teaming Large Language Model Assistants with Relaxing Constraints},
    volume={39},
    url={https://ojs.aaai.org/index.php/AAAI/article/view/34797},
    DOI={10.1609/aaai.v39i24.34797},
    abstract={Recent advances in large language model assistants have made them indispensable, raising significant concerns over managing their safety. Automated red teaming offers a promising alternative to the labor-intensive and error-prone manual probing for vulnerabilities, providing more consistent and scalable safety evaluations. However, existing approaches often compromise diversity by focusing on maximizing attack success rate. Additionally, methods that decrease the cosine similarity from historical embeddings with semantic diversity rewards lead to novelty stagnation as history grows. To address these issues, we introduce DiveR-CT, which relaxes conventional constraints on the objective and semantic reward, granting greater freedom for the policy to enhance diversity. Our experiments demonstrate DiveR-CT’s marked superiority over baselines by 1) generating data that perform better in various diversity metrics across different attack success rate levels, 2) better-enhancing resiliency in blue team models through safety tuning based on collected data, 3) allowing dynamic control of objective weights for reliable and controllable attack success rates, and 4) reducing susceptibility to reward overoptimization. Overall, our method provides an effective and efficient approach to LLM red teaming, accelerating real-world deployment. WARNING: This paper contains examples of potentially harmful text.},
    number={24},
    journal={AAAI},
    author={Zhao, Andrew and Xu, Quentin and Lin, Matthieu and Wang, Shenzhi and Liu, Yong-Jin and Zheng#, Zilong and Huang#, Gao},
    year={2025},
    pages={26021-26030},
    award={Oral},
    arxiv={2405.19026},
    abbr={AAAI'25},
    code={https://github.com/LeapLabTHU/diver-ct},
    website={https://andrewzh112.github.io/diver-ct/},
}

@inproceedings{qi2025ice,
    title={In-Context Editing: Learning Knowledge from Self-Induced Distributions},
    author={Siyuan Qi# and Bangcheng Yang and Kailin Jiang and Xiaobo Wang and Jiaqi Li and Yifan Zhong and Yaodong Yang and Zilong Zheng#},
    year={2025},
    arxiv={2406.11194},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2406.11194},
    booktitle={ICLR},
    abbr={ICLR'25},
    code={https://github.com/bigai-ai/ICE},
    abstract={The existing fine-tuning paradigm for language models is brittle in knowledge editing scenarios, where the model must incorporate new information without extensive retraining. This brittleness often results in overfitting, reduced performance, and unnatural language generation. To address this, we propose Consistent In-Context Editing (ICE), a novel approach that leverages the model's in-context learning capability to tune toward a contextual distribution rather than a one-hot target. ICE introduces a straightforward optimization framework that includes both a target and a procedure, enhancing the robustness and effectiveness of gradient-based tuning methods. We provide analytical insights into ICE across four critical aspects of knowledge editing: accuracy, locality, generalization, and linguistic quality, showing its advantages. Experimental results across four datasets confirm the effectiveness of ICE and demonstrate its potential for continual editing, ensuring that updated information is incorporated while preserving the integrity of the model.}
}

@inproceedings{zhang2025amulet,
    title={Amulet: ReAlignment During Test Time for Personalized Preference Adaptation of LLMs},
    author={Zhang, Zhaowei and Bai, Fengshuo and Chen, Qizhi and Ma, Chengdong and Wang, Mingzhi and Sun, Haoran and Zheng#, Zilong and Yang#, Yaodong},
    year={2025},
    arxiv={2502.19148},
    pdf={https://openreview.net/forum?id=f9w89OY2cp},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    booktitle={ICLR},
    abbr={ICLR'25},
    code={https://github.com/zowiezhang/Amulet},
    abstract={How to align large language models (LLMs) with user preferences from a static general dataset has been frequently studied. However, user preferences are usually personalized, changing, and diverse. This leads to the problem that the actual user preferences often do not coincide with those trained by the model developers in the practical use of LLMs. Since we cannot collect enough data and retrain for every demand, researching efficient real-time preference adaptation methods based on the backbone LLMs during test time is important. To this end, we introduce Amulet, a novel, training-free framework that formulates the decoding process of every token as a separate online learning problem with the guidance of simple user-provided prompts, thus enabling real-time optimization to satisfy users' personalized preferences. To reduce the computational cost brought by this optimization process for each token, we additionally provide a closed-form solution for each iteration step of the optimization process, thereby reducing the computational time cost to a negligible level. The detailed experimental results demonstrate that Amulet can achieve significant performance improvements in rich settings with combinations of different LLMs, datasets, and user preferences, while maintaining acceptable computational efficiency.}
}

@article{li2025latentseek,
      title={Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space},
      author={Hengli Li and Chenxi Li and Tong Wu and Xuekai Zhu and Yuxuan Wang and Zhaoxin Yu and Eric Hanchen Jiang and Song-Chun Zhu and Zixia Jia and Ying Nian Wu and Zilong Zheng},
      year={2025},
      arxiv={2505.13308},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2505.13308},
      journal={Preprint},
      code={https://github.com/bigai-nlco/LatentSeek},
      website={https://bigai-nlco.github.io/LatentSeek/},
      abstract={Reasoning ability, a core component of human intelligence, continues to pose a significant challenge for Large Language Models (LLMs) in the pursuit of AGI. Although model performance has improved under the training scaling law, significant challenges remain, particularly with respect to training algorithms, such as catastrophic forgetting, and the limited availability of novel training data. As an alternative, test-time scaling enhances reasoning performance by increasing test-time computation without parameter updating. Unlike prior methods in this paradigm focused on token space, we propose leveraging latent space for more effective reasoning and better adherence to the test-time scaling law. We introduce LatentSeek, a novel framework that enhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA) within the model's latent space. Specifically, LatentSeek leverages policy gradient to iteratively update latent representations, guided by self-generated reward signals. LatentSeek is evaluated on a range of reasoning benchmarks, including GSM8K, MATH-500, and AIME2024, across multiple LLM architectures. Results show that LatentSeek consistently outperforms strong baselines, such as Chain-of-Thought prompting and fine-tuning-based methods. Furthermore, our analysis demonstrates that LatentSeek is highly efficient, typically converging within a few iterations for problems of average complexity, while also benefiting from additional iterations, thereby highlighting the potential of test-time scaling in the latent space. These findings position LatentSeek as a lightweight, scalable, and effective solution for enhancing the reasoning capabilities of LLMs.}
}

@inproceedings{du2025mmke,
    title={MMKE-Bench: A Multimodal Editing Benchmark for Diverse Visual Knowledge},
    author={Du, Yuntao and Jiang, Kailin and Gao, Zhi and Shi, Chenrui and Zheng#, Zilong and Qi, Siyuan and Li#, Qing},
    year={2025},
    arxiv={2502.19870},
    pdf={https://openreview.net/forum?id=v8qABSeeKO},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    booktitle=iclr,
    abbr={ICLR'25},
    code={https://github.com/zowiezhang/Amulet},
    abstract={How to align large language models (LLMs) with user preferences from a static general dataset has been frequently studied. However, user preferences are usually personalized, changing, and diverse. This leads to the problem that the actual user preferences often do not coincide with those trained by the model developers in the practical use of LLMs. Since we cannot collect enough data and retrain for every demand, researching efficient real-time preference adaptation methods based on the backbone LLMs during test time is important. To this end, we introduce Amulet, a novel, training-free framework that formulates the decoding process of every token as a separate online learning problem with the guidance of simple user-provided prompts, thus enabling real-time optimization to satisfy users' personalized preferences. To reduce the computational cost brought by this optimization process for each token, we additionally provide a closed-form solution for each iteration step of the optimization process, thereby reducing the computational time cost to a negligible level. The detailed experimental results demonstrate that Amulet can achieve significant performance improvements in rich settings with combinations of different LLMs, datasets, and user preferences, while maintaining acceptable computational efficiency.}
}

@inproceedings{wang2025omnimmi,
      title={OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming Video Contexts},
      author={Yuxuan Wang and Yueqian Wang and Bo Chen and Tong Wu and Dongyan Zhao and Zilong Zheng#},
      year={2025},
      arxiv={2503.22952},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2503.22952},
      code={https://github.com/OmniMMI/OmniMMI},
      website={https://omnimmi.github.io/},
      abstract={The rapid advancement of multi-modal language models (MLLMs) like GPT-4o has propelled the development of Omni language models, designed to process and proactively respond to continuous streams of multi-modal data. Despite their potential, evaluating their real-world interactive capabilities in streaming video contexts remains a formidable challenge. In this work, we introduce OmniMMI, a comprehensive multi-modal interaction benchmark tailored for OmniLLMs in streaming video contexts. OmniMMI encompasses over 1,121 real-world interactive videos and 2,290 questions, addressing two critical yet underexplored challenges in existing video benchmarks: streaming video understanding and proactive reasoning, across six distinct subtasks. Moreover, we propose a novel framework, Multi-modal Multiplexing Modeling (M4), designed to enhance real-time interactive reasoning with minimum finetuning on pre-trained MLLMs. Extensive experimental results reveal that the existing MLLMs fall short in interactive streaming understanding, particularly struggling with proactive tasks and multi-turn queries. Our proposed M4, though lightweight, demonstrates a significant improvement in handling proactive tasks and real-time interactions.},
      booktitle={CVPR},
      abbr={CVPR'25},
}
