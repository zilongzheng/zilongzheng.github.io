<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Zilong Zheng's Homepage </title> <meta name="author" content="Zilong Zheng"> <meta name="description" content="Zilong Zheng's Homepage. "> <meta name="keywords" content="AI Researcher, NLPer, MLer"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?16404ec2cd2689e8d0f38f73fe0d38f9"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?05a47794b559753970d2608ea15c4666"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Zilong</span> Zheng </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/" style="text-transform: capitalize;">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/" style="text-transform: capitalize;">publication </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span style="text-transform: uppercase;">Zilong Zheng</span> </h1> <p class="desc"></p> <p>Email: z.zheng<code>[at]</code>ucla<code>[dot]</code>edu</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/zilong.JPG" sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/zilong.JPG?e28dbc43afe25d0181a08cec37d644fe" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="zilong.JPG" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%7A.%7A%68%65%6E%67@%75%63%6C%61.%65%64%75" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=9sDx70IAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/zilongzheng" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/zilongzheng0318" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://twitter.com/ZilongZheng" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> </div> </div> <div class="clearfix"> <p>I received my Ph.D. degree (21’) from the Department of Computer Science at <a href="https://www.cs.ucla.edu/" rel="external nofollow noopener" target="_blank">University of California, Los Angeles (UCLA)</a>. My research interests lie in the intersection of statistical machine learning, natural language processing and cognition. Current research themes include: </p> <ul> <li> <strong>Human AI Alignment</strong>: Building interactive models that align with human values and social norms.</li> <li> <strong>Long-context Language Models:</strong> Efficient training and inference of long-context language models.</li> <li> <strong>Generative Modeling:</strong> Statistical generative modeling (e.g. EBMs, diffusions) on high-dimensional data.</li> </ul> <p>I am always looking for self-motivated <strong>students</strong> and long-term <strong>collaborators</strong>. Please contact me if you have excellent background or share similar research interests with me.</p> </div> <div style="border-top: 1px solid var(--global-divider-color); margin: 1rem 0 2rem 0;"></div> <h2> <a href="/news/" style="color: inherit; text-transform:uppercase;">NEWS</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">May, 2025</th> <td> I will be serving as Senior Area Chair for EMNLP. </td> </tr> <tr> <th scope="row" style="width: 20%">May, 2025</th> <td> <img class="emoji" title=":fire:" alt=":fire:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png" height="20" width="20"> <b>Three</b> papers on <a href="publication#bidirectionalencoder">bidirectional LLM Encoder</a>, <a href="publication#reflectevo"><strong>ReflectEvo</strong> (Meta Reflection Learning)</a> and <a href="publication#valuesteering">Causal Value Steering</a> are accepted to <strong>ACL’25</strong>! One paper on <a href="publication#creativity">combinational creativity in VLMs</a> is accepted to <strong>CogSci’25</strong> for Oral presentation! Congratulations to Ziyong, Jiaqi, Yipeng and Yongqian! </td> </tr> <tr> <th scope="row" style="width: 20%">May, 2025</th> <td> <img class="emoji" title=":fire:" alt=":fire:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png" height="20" width="20"> <b>Three</b> papers on <a href="publication#tokenswift"><strong>TokenSwift</strong> (long sequence acceleration)</a>, <a href="publication#toedit"><strong>ToEdit</strong> (LLM model collapse)</a> and <a href="publication#mcu"><strong>MCU</strong> (open-ended agent evaluation)</a> are accepted to <strong>ICML’25</strong>! <a href="publication#mcu">MCU</a> is awarded as <span style="color: #990000">Spotlight</span> Poster! Congratulations to Tong, Xuekai and Xinyue! </td> </tr> <tr> <th scope="row" style="width: 20%">Mar, 2025</th> <td> <a href="publication/#omnimmi">OmniMMI</a> is accepted to <strong>CVPR’25</strong> <img class="emoji" title=":fire:" alt=":fire:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png" height="20" width="20">. We devised the <strong>first-ever</strong> benchmark for streaming interactive Omni understanding. Please try your models on <a href="https://omnimmi.github.io/" rel="external nofollow noopener" target="_blank">OmniMMI Leaderboard</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Jan, 2025</th> <td> <b>Three</b> papers on <a href="publication#iclr25ice">in-context knowledge editing</a>, <a href="publication#iclr25mmke">multimodal knowledge editing</a> and <a href="publication#iclr25amulet">in-context alignment</a> are accepted to <strong>ICLR’25</strong>! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20"> </td> </tr> </table> </div> </div> <div style="border-top: 1px solid var(--global-divider-color); margin: 1rem 0 2rem 0;"></div> <h2> <a href="/publications/" style="color: inherit; text-transform:uppercase;">selected publications</a> </h2> <div class="publications"> <div class="alert alert-primary"><a href="/publications">SEE ALL PUBLICATIONS <i class="fas fa-angle-right" aria-hidden="true"></i></a></div> <ol class="bibliography"> <li> <div class="row"> <div id="zhu2025toedit" class="col-sm-10"> <div class="title">How to Synthesize Text Data without Model Collapse? <span class="abbr"> <abbr class="badge rounded">ICML'25</abbr> </span> </div> <div class="author"> <a href="https://xuekai-zhu.github.io/Xuekai-Zhu/" rel="external nofollow noopener" target="_blank">Xuekai Zhu</a>, Daixuan Cheng, Hengli Li, Kaiyan Zhang, Ermo Hua, Xingtai Lv, Ning Ding, <a href="https://hantek.github.io/" rel="external nofollow noopener" target="_blank">Zhouhan Lin<sup>#</sup></a>, <span style="font-weight:600">Zilong Zheng<sup>#</sup></span>, and <a href="https://c3i.ee.tsinghua.edu.cn/en/author/bowen-zhou/" rel="external nofollow noopener" target="_blank">Bowen Zhou<sup>#</sup></a><span class="periodical">, <em>in ICML</em>, 2025. </span> <div class="periodical"> </div> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2412.14689" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://x.com/ZilongZheng/status/1870366949907349576" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">X</a> </div> <div class="abstract hidden"> <p>Model collapse in synthetic data indicates that iterative training on self-generated data leads to a gradual decline in performance. With the proliferation of AI models, synthetic data will fundamentally reshape the web data ecosystem. Future GPT-{n} models will inevitably be trained on a blend of synthetic and human-produced data. In this paper, we focus on two questions: what is the impact of synthetic data on language model training, and how to synthesize data without model collapse? We first pre-train language models across different proportions of synthetic data, revealing a negative correlation between the proportion of synthetic data and model performance. We further conduct statistical analysis on synthetic data to uncover distributional shift phenomenon and over-concentration of n-gram features. Inspired by the above findings, we propose token editing on human-produced data to obtain semi-synthetic data. As a proof of concept, we theoretically demonstrate that token-level editing can prevent model collapse, as the test error is constrained by a finite upper bound. We conduct extensive experiments on pre-training from scratch, continual pre-training, and supervised fine-tuning. The results validate our theoretical proof that token-level editing improves data quality and enhances model performance. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zhu2025toedit</span><span class="p">,</span>
    <span class="na">title</span><span class="p">=</span><span class="s">{How to Synthesize Text Data without Model Collapse?}</span><span class="p">,</span>
    <span class="na">author</span><span class="p">=</span><span class="s">{Zhu, Xuekai and Cheng, Daixuan and Li, Hengli and Zhang, Kaiyan and Hua, Ermo and Lv, Xingtai and Ding, Ning and Lin, Zhouhan and Zheng, Zilong and Zhou, Bowen}</span><span class="p">,</span>
    <span class="na">journal</span> <span class="p">=</span> <span class="s">{Forty-Second International Conference on Machine Learning}</span><span class="p">,</span>
    <span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="zheng2025mcu" class="col-sm-10"> <div class="title">MCU: An Evaluation Framework for Open-Ended Game Agents <span class="abbr"> <abbr class="badge rounded">ICML'25</abbr> </span> <span class="abbr"> <abbr class="badge rounded award">Spotlight Paper</abbr> </span> </div> <div class="author"> Xinyue Zheng<sup>*</sup>, Haowei Lin<sup>*</sup>, Kaichen He, Zihao Wang, <span style="font-weight:600">Zilong Zheng<sup>#</sup></span>, and <a href="https://web.cs.ucla.edu/~yliang/" rel="external nofollow noopener" target="_blank">Yitao Liang<sup>#</sup></a><span class="periodical">, <em>in ICML</em>, 2025. </span> <div class="periodical"> </div> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2310.08367" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/CraftJarvis/MCU" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Developing AI agents capable of interacting with open-world environments to solve diverse tasks is a compelling challenge. However, evaluating such open-ended agents remains difficult, with current benchmarks facing scalability limitations. To address this, we introduce <em>Minecraft Universe</em> (MCU), a comprehensive evaluation framework set within the open-world video game Minecraft. MCU incorporates three key components: (1) an expanding collection of 3,452 composable atomic tasks that encompasses 11 major categories and 41 subcategories of challenges; (2) a task composition mechanism capable of generating infinite diverse tasks with varying difficulty; and (3) a general evaluation framework that achieves 91.5% alignment with human ratings for open-ended task assessment. Empirical results reveal that even state-of-the-art foundation agents struggle with the increasing diversity and complexity of tasks. These findings highlight the necessity of MCU as a robust benchmark to drive progress in AI agent development within open-ended environments. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zheng2025mcu</span><span class="p">,</span>
    <span class="na">title</span><span class="p">=</span><span class="s">{MCU: An Evaluation Framework for Open-Ended Game Agents}</span><span class="p">,</span>
    <span class="na">author</span><span class="p">=</span><span class="s">{Zheng, Xinyue and Lin, Haowei and He, Kaichen and Wang, Zihao and Zheng, Zilong and Liang, Yitao}</span><span class="p">,</span>
    <span class="na">journal</span> <span class="p">=</span> <span class="s">{Forty-Second International Conference on Machine Learning}</span><span class="p">,</span>
    <span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="wu2025tokenswift" class="col-sm-10"> <div class="title">Lossless Acceleration of Ultra Long Sequence Generation <span class="abbr"> <abbr class="badge rounded">ICML'25</abbr> </span> </div> <div class="author"> <a href="https://wutong4012.github.io/" rel="external nofollow noopener" target="_blank">Tong Wu</a>, Junzhe Shen, <a href="https://jzxxx.github.io/" rel="external nofollow noopener" target="_blank">Zixia Jia</a>, <a href="https://patrick-tssn.github.io/" rel="external nofollow noopener" target="_blank">Yuxuan Wang</a>, and <span style="font-weight:600">Zilong Zheng<sup>#</sup></span><span class="periodical">, <em>in ICML</em>, 2025. </span> <div class="periodical"> </div> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2502.18890" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/bigai-nlco/TokenSwift" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://x.com/ZilongZheng/status/1896843753446609382" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">X</a> </div> <div class="badges"> </div> <a href="https://github.com/bigai-nlco/TokenSwift" aria-label="GitHub link" role="button" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/bigai-nlco/TokenSwift"> </a> <div class="abstract hidden"> <p>Generating ultra-long sequences with large language models (LLMs) has become increasingly crucial but remains a highly time-intensive task, particularly for sequences up to 100K tokens. While traditional speculative decoding methods exist, simply extending their generation limits fails to accelerate the process and can be detrimental. Through an in-depth analysis, we identify three major challenges hindering efficient generation: frequent model reloading, dynamic key-value (KV) management and repetitive generation. To address these issues, we introduce TOKENSWIFT, a novel framework designed to substantially accelerate the generation process of ultra-long sequences while maintaining the target model's inherent quality. Experimental results demonstrate that TOKENSWIFT achieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B, 14B) and architectures (MHA, GQA). This acceleration translates to hours of time savings for ultra-long sequence generation, establishing TOKENSWIFT as a scalable and effective solution at unprecedented lengths. Code can be found at <a href="https://github.com/bigai-nlco/TokenSwift" rel="external nofollow noopener" target="_blank">this URL</a>. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">wu2025tokenswift</span><span class="p">,</span>
    <span class="na">title</span><span class="p">=</span><span class="s">{Lossless Acceleration of Ultra Long Sequence Generation}</span><span class="p">,</span>
    <span class="na">author</span><span class="p">=</span><span class="s">{Wu, Tong and Shen, Junzhe and Jia, Zixia and Wang, Yuxuan and Zheng, Zilong}</span><span class="p">,</span>
    <span class="na">journal</span> <span class="p">=</span> <span class="s">{Forty-Second International Conference on Machine Learning}</span><span class="p">,</span>
    <span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="wang2025omnimmi" class="col-sm-10"> <div class="title">OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming Video Contexts <span class="abbr"> <abbr class="badge rounded">CVPR'25</abbr> </span> </div> <div class="author"> <a href="https://patrick-tssn.github.io/" rel="external nofollow noopener" target="_blank">Yuxuan Wang</a>, Yueqian Wang, Bo Chen, <a href="https://wutong4012.github.io/" rel="external nofollow noopener" target="_blank">Tong Wu</a>, <a href="https://www.wict.pku.edu.cn/zhaodongyan/en/" rel="external nofollow noopener" target="_blank">Dongyan Zhao</a>, and <span style="font-weight:600">Zilong Zheng<sup>#</sup></span><span class="periodical">, <em>in CVPR</em>, 2025. </span> <div class="periodical"> </div> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2503.22952" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/OmniMMI/OmniMMI" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://omnimmi.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>The rapid advancement of multi-modal language models (MLLMs) like GPT-4o has propelled the development of Omni language models, designed to process and proactively respond to continuous streams of multi-modal data. Despite their potential, evaluating their real-world interactive capabilities in streaming video contexts remains a formidable challenge. In this work, we introduce OmniMMI, a comprehensive multi-modal interaction benchmark tailored for OmniLLMs in streaming video contexts. OmniMMI encompasses over 1,121 real-world interactive videos and 2,290 questions, addressing two critical yet underexplored challenges in existing video benchmarks: streaming video understanding and proactive reasoning, across six distinct subtasks. Moreover, we propose a novel framework, Multi-modal Multiplexing Modeling (M4), designed to enhance real-time interactive reasoning with minimum finetuning on pre-trained MLLMs. Extensive experimental results reveal that the existing MLLMs fall short in interactive streaming understanding, particularly struggling with proactive tasks and multi-turn queries. Our proposed M4, though lightweight, demonstrates a significant improvement in handling proactive tasks and real-time interactions. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">cvpr25omnimmi</span><span class="p">,</span>
    <span class="na">title</span><span class="p">=</span><span class="s">{OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming Video Contexts}</span><span class="p">,</span>
    <span class="na">author</span><span class="p">=</span><span class="s">{Wang, Yuxuan and Wang, Yueqian and Chen, Bo and Wu, Tong and Zhao, Dongyan and Zheng, Zilong}</span><span class="p">,</span>
    <span class="na">booktitle</span><span class="p">=</span><span class="s">{Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)}</span><span class="p">,</span>
    <span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="zhao2025absolute" class="col-sm-10"> <div class="title">Absolute Zero: Reinforced Self-play Reasoning with Zero Data </div> <div class="author"> <a href="https://andrewzh112.github.io/" rel="external nofollow noopener" target="_blank">Andrew Zhao</a>, Yiran Wu, Yang Yue, <a href="https://wutong4012.github.io/" rel="external nofollow noopener" target="_blank">Tong Wu</a>, Quentin Xu, Yang Yue, Matthieu Lin, Shenzhi Wang, Qingyun Wu, <span style="font-weight:600">Zilong Zheng<sup>#</sup></span>, and <a href="http://www.gaohuang.net/" rel="external nofollow noopener" target="_blank">Gao Huang<sup>#</sup></a><span class="periodical">, <em>Preprint</em>, 2025. </span> <div class="periodical"> </div> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2505.03335" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/LeapLabTHU/Absolute-Zero-Reasoner" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://andrewzh112.github.io/absolute-zero-reasoner/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a href="https://huggingface.co/collections/andrewzh/absolute-zero-reasoner-68139b2bca82afb00bc69e5b" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Model</a> <a href="https://x.com/_AndrewZhao/status/1919920459748909288" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">X</a> <a href="https://huggingface.co/papers/2505.03335" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HF Papers</a> </div> <div class="badges"> </div> <a href="https://github.com/LeapLabTHU/Absolute-Zero-Reasoner" aria-label="GitHub link" role="button" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/LeapLabTHU/Absolute-Zero-Reasoner"> </a> <div class="abstract hidden"> <p>Model collapse in synthetic data indicates that iterative training on self-generated data leads to a gradual decline in performance. With the proliferation of AI models, synthetic data will fundamentally reshape the web data ecosystem. Future GPT-{n} models will inevitably be trained on a blend of synthetic and human-produced data. In this paper, we focus on two questions: what is the impact of synthetic data on language model training, and how to synthesize data without model collapse? We first pre-train language models across different proportions of synthetic data, revealing a negative correlation between the proportion of synthetic data and model performance. We further conduct statistical analysis on synthetic data to uncover distributional shift phenomenon and over-concentration of n-gram features. Inspired by the above findings, we propose token editing on human-produced data to obtain semi-synthetic data. As a proof of concept, we theoretically demonstrate that token-level editing can prevent model collapse, as the test error is constrained by a finite upper bound. We conduct extensive experiments on pre-training from scratch, continual pre-training, and supervised fine-tuning. The results validate our theoretical proof that token-level editing improves data quality and enhances model performance. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">zhao2025absolutezeroreinforcedselfplay</span><span class="p">,</span>
      <span class="na">title</span><span class="p">=</span><span class="s">{Absolute Zero: Reinforced Self-play Reasoning with Zero Data}</span><span class="p">,</span>
      <span class="na">author</span><span class="p">=</span><span class="s">{Andrew Zhao and Yiran Wu and Yang Yue and Tong Wu and Quentin Xu and Yang Yue and Matthieu Lin and Shenzhi Wang and Qingyun Wu and Zilong Zheng and Gao Huang}</span><span class="p">,</span>
      <span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
      <span class="na">eprint</span><span class="p">=</span><span class="s">{2505.03335}</span><span class="p">,</span>
      <span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
      <span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.LG}</span><span class="p">,</span>
      <span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2505.03335}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="li2025latentseek" class="col-sm-10"> <div class="title">Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space </div> <div class="author"> Hengli Li, Chenxi Li, <a href="https://wutong4012.github.io/" rel="external nofollow noopener" target="_blank">Tong Wu</a>, <a href="https://xuekai-zhu.github.io/Xuekai-Zhu/" rel="external nofollow noopener" target="_blank">Xuekai Zhu</a>, <a href="https://patrick-tssn.github.io/" rel="external nofollow noopener" target="_blank">Yuxuan Wang</a>, Zhaoxin Yu, Eric Hanchen Jiang, <a href="http://www.stat.ucla.edu/~sczhu/" rel="external nofollow noopener" target="_blank">Song-Chun Zhu</a>, <a href="https://jzxxx.github.io/" rel="external nofollow noopener" target="_blank">Zixia Jia</a>, <a href="http://www.stat.ucla.edu/~ywu/research.html" rel="external nofollow noopener" target="_blank">Ying Nian Wu<sup>#</sup></a>, and <span style="font-weight:600">Zilong Zheng<sup>#</sup></span><span class="periodical">, <em>Preprint</em>, 2025. </span> <div class="periodical"> </div> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2505.13308" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/bigai-nlco/LatentSeek" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://bigai-nlco.github.io/LatentSeek/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Reasoning ability, a core component of human intelligence, continues to pose a significant challenge for Large Language Models (LLMs) in the pursuit of AGI. Although model performance has improved under the training scaling law, significant challenges remain, particularly with respect to training algorithms, such as catastrophic forgetting, and the limited availability of novel training data. As an alternative, test-time scaling enhances reasoning performance by increasing test-time computation without parameter updating. Unlike prior methods in this paradigm focused on token space, we propose leveraging latent space for more effective reasoning and better adherence to the test-time scaling law. We introduce LatentSeek, a novel framework that enhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA) within the model's latent space. Specifically, LatentSeek leverages policy gradient to iteratively update latent representations, guided by self-generated reward signals. LatentSeek is evaluated on a range of reasoning benchmarks, including GSM8K, MATH-500, and AIME2024, across multiple LLM architectures. Results show that LatentSeek consistently outperforms strong baselines, such as Chain-of-Thought prompting and fine-tuning-based methods. Furthermore, our analysis demonstrates that LatentSeek is highly efficient, typically converging within a few iterations for problems of average complexity, while also benefiting from additional iterations, thereby highlighting the potential of test-time scaling in the latent space. These findings position LatentSeek as a lightweight, scalable, and effective solution for enhancing the reasoning capabilities of LLMs. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">li2025seekdarkreasoningtesttime</span><span class="p">,</span>
      <span class="na">title</span><span class="p">=</span><span class="s">{Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space}</span><span class="p">,</span>
      <span class="na">author</span><span class="p">=</span><span class="s">{Hengli Li and Chenxi Li and Tong Wu and Xuekai Zhu and Yuxuan Wang and Zhaoxin Yu and Eric Hanchen Jiang and Song-Chun Zhu and Zixia Jia and Ying Nian Wu and Zilong Zheng}</span><span class="p">,</span>
      <span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
      <span class="na">eprint</span><span class="p">=</span><span class="s">{2505.13308}</span><span class="p">,</span>
      <span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
      <span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.LG}</span><span class="p">,</span>
      <span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2505.13308}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="NeurIPS24-CREAM" class="col-sm-10"> <div class="title">An Efficient Recipe for Long Context Extension via Middle-Focused Positional Encoding <span class="abbr"> <abbr class="badge rounded">NeurIPS'24</abbr> </span> </div> <div class="author"> <a href="https://wutong4012.github.io/" rel="external nofollow noopener" target="_blank">Tong Wu</a>, Yanpeng Zhao, and <span style="font-weight:600">Zilong Zheng<sup>#</sup></span><span class="periodical">, <em>in NeurIPS</em>, 2024. </span> <div class="periodical"> </div> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2406.07138v1" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/bigai-nlco/cream" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Recently, many methods have been developed to extend the context length of pre-trained large language models (LLMs), but they often require fine-tuning at the target length (&gt;&gt; 4K) and struggle to effectively utilize information from the middle part of the context. To address these issues, we propose <b>C</b>ontinuity-<b>R</b>elativity ind<b>E</b>xing with g<b>A</b>ussian <b>M</b>iddle (CREAM), which interpolates positional encodings by manipulating position indices. Apart from being simple, CREAM is training-efficient: it only requires fine-tuning at the pre-trained context window (e.g., Llama 2-4K) and can extend LLMs to a much longer target context length (e.g., 256K). To ensure that the model focuses more on the information in the middle, we introduce a truncated Gaussian to encourage sampling from the middle part of the context during fine-tuning, thus alleviating the "Lost-in-the-Middle" problem faced by long-context LLMs. Experimental results show that CREAM successfully extends LLMs to the target length for both Base and Chat versions of Llama2-7B with "Never Miss A Beat". </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wu2024cream</span><span class="p">,</span>
    <span class="na">title</span><span class="p">=</span><span class="s">{An Efficient Recipe for Long Context Extension via Middle-Focused Positional Encoding}</span><span class="p">,</span>
    <span class="na">author</span><span class="p">=</span><span class="s">{Tong Wu, Yanpeng Zhao, Zilong Zheng}</span><span class="p">,</span>
    <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems (NeurIPS)}</span><span class="p">,</span>
    <span class="na">volume</span> <span class="p">=</span> <span class="s">{37}</span><span class="p">,</span>
    <span class="na">year</span><span class="p">=</span><span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="bidirectional22scirob" class="col-sm-10"> <div class="title">In situ bidirectional human-robot value alignment <span class="abbr"> <abbr class="badge rounded">ScienceRobotics</abbr> </span> </div> <div class="author"> <a href="https://yuanluya.github.io/" rel="external nofollow noopener" target="_blank">Luyao Yuan<sup>*#</sup></a>, <a href="https://xfgao.github.io/" rel="external nofollow noopener" target="_blank">Xiaofeng Gao<sup>*</sup></a>, <span style="font-weight:600">Zilong Zheng<sup>*</sup></span>, <a href="https://mjedmonds.com/" rel="external nofollow noopener" target="_blank">Mark Edmonds</a>, <a href="http://www.stat.ucla.edu/~ywu/research.html" rel="external nofollow noopener" target="_blank">Ying Nian Wu</a>, <a href="https://scholar.google.com/citations?user=tkbxHjsAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Federico Rossano</a>, <a href="https://www.psych.ucla.edu/faculty-page/hongjing/" rel="external nofollow noopener" target="_blank">Hongjing Lu<sup>#</sup></a>, <a href="https://yzhu.io/" rel="external nofollow noopener" target="_blank">Yixin Zhu<sup>#</sup></a>, and <a href="http://www.stat.ucla.edu/~sczhu/" rel="external nofollow noopener" target="_blank">Song-Chun Zhu<sup>#</sup></a><span class="periodical">, <em>Science Robotics</em>, 2022. </span> <div class="periodical"> </div> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.science.org/action/downloadSupplement?doi=10.1126%2Fscirobotics.abm4183&amp;file=scirobotics.abm4183_sm.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Supp</a> <a href="https://vimeo.com/730025438" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://doi.org/10.5068/D1XT3V" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://yzhu.io/publication/teaming2022scirob/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a href="https://techxplore.com/news/2022-08-ai-paradigm-human-robot-collaboration.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">TechXplore</a> <a href="http://www.xinhuanet.com/tech/20220714/4d46925b0def47f0914aae9c030bd36b/c.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">科技日报/新华网</a> </div> <div class="abstract hidden"> <p>A prerequisite for social coordination is bidirectional communication between teammates, each playing two roles simultaneously: as receptive listeners and expressive speakers. For robots working with humans in complex situations with multiple goals that differ in importance, failure to fulfill the expectation of either role could undermine group performance due to misalignment of values between humans and robots. Specifically, a robot needs to serve as an effective listener to infer human users’ intents from instructions and feedback and as an expressive speaker to explain its decision processes to users. Here, we investigate how to foster effective bidirectional human-robot communications in the context of value alignment—collaborative robots and users form an aligned understanding of the importance of possible task goals. We propose an explainable artificial intelligence (XAI) system in which a group of robots predicts users’ values by taking in situ feedback into consideration while communicating their decision processes to users through explanations. To learn from human feedback, our XAI system integrates a cooperative communication model for inferring human values associated with multiple desirable goals. To be interpretable to humans, the system simulates human mental dynamics and predicts optimal explanations using graphical models. We conducted psychological experiments to examine the core components of the proposed computational framework. Our results show that real-time human-robot mutual understanding in complex cooperative tasks is achievable with a learning model based on bidirectional communication. We believe that this interaction framework can shed light on bidirectional value alignment in communicative XAI systems and, more broadly, in future human-machine teaming systems. An explainable artificial intelligence collaboration framework enables in situ bidirectional human-robot value alignment. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span>
    <span class="nl">doi:10.1126/scirobotics.abm4183</span><span class="p">,</span>
    <span class="na">author</span> <span class="p">=</span> <span class="s">{Luyao Yuan  and Xiaofeng Gao  and Zilong Zheng  and Mark Edmonds  and Ying Nian Wu  and Federico Rossano  and Hongjing Lu  and Yixin Zhu  and Song-Chun Zhu }</span><span class="p">,</span>
    <span class="na">title</span> <span class="p">=</span> <span class="s">{In situ bidirectional human-robot value alignment}</span><span class="p">,</span>
    <span class="na">journal</span> <span class="p">=</span> <span class="s">{Science Robotics}</span><span class="p">,</span>
    <span class="na">volume</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
    <span class="na">number</span> <span class="p">=</span> <span class="s">{68}</span><span class="p">,</span>
    <span class="na">pages</span> <span class="p">=</span> <span class="s">{eabm4183}</span><span class="p">,</span>
    <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
    <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1126/scirobotics.abm4183}</span><span class="p">,</span>
    <span class="na">URL</span> <span class="p">=</span> <span class="s">{https://www.science.org/doi/abs/10.1126/scirobotics.abm4183}</span><span class="p">,</span>
    <span class="na">eprint</span> <span class="p">=</span> <span class="s">{https://www.science.org/doi/pdf/10.1126/scirobotics.abm4183}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="zheng2021patchgconvNet" class="col-sm-10"> <div class="title">Patchwise Generative ConvNet: Training Energy-Based Models from a Single Natural Image for Internal Learning <span class="abbr"> <abbr class="badge rounded">CVPR'21</abbr> </span> <span class="abbr"> <abbr class="badge rounded award">Oral</abbr> </span> </div> <div class="author"> <span style="font-weight:600">Zilong Zheng</span>, <a href="http://www.stat.ucla.edu/~jxie/" rel="external nofollow noopener" target="_blank">Jianwen Xie</a>, and <a href="http://research.baidu.com/People/index-view?id=111" rel="external nofollow noopener" target="_blank">Ping Li</a><span class="periodical">, <em>in CVPR</em>, 2021. </span> <div class="periodical"> </div> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/PatchGenCN/CVPR21_PatchGenCN.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/PatchGenCN/CVPR21_PatchGenCN_supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://github.com/zilongzheng/PatchGenCN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://zilongzheng.github.io/PatchGenCN/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Exploiting internal statistics of a single natural image has long been recognized as a significant research paradigm where the goal is to learn the internal distribution of patches within the image without relying on external training data. Different from prior works that model such a distribution implicitly with a top-down latent variable model (e.g., generator), this paper proposes to explicitly represent the statistical distribution within a single natural image by using an energy-based generative framework, where a pyramid of energy functions, each parameterized by a bottom-up deep neural network, are used to capture the distributions of patches at different resolutions. Meanwhile, a coarse-to-fine sequential training and sampling strategy is presented to train the model efficiently. Besides learning to generate random samples from white noise, the model can learn in parallel with a self-supervised task (e.g., recover the input image from its corrupted version), which can further improve the descriptive power of the learned model. The proposed model is simple and natural in that it does not require an auxiliary model (e.g., discriminator) to assist the training. Besides, it also unifies internal statistics learning and image generation in a single framework. Experimental results presented on various image generation and manipulation tasks, including super-resolution, image editing, harmonization, style transfer, etc., have demonstrated the effectiveness of our model for internal learning. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zheng2021patchgencn</span><span class="p">,</span>
    <span class="na">title</span><span class="p">=</span><span class="s">{Patchwise Generative ConvNet: Training Energy-Based Models from a Single Natural Image for Internal Learning}</span><span class="p">,</span>
    <span class="na">author</span><span class="p">=</span><span class="s">{Zheng, Zilong and Xie, Jianwen and Li, Ping}</span><span class="p">,</span>
    <span class="na">booktitle</span><span class="p">=</span><span class="s">{Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)}</span><span class="p">,</span>
    <span class="na">year</span><span class="p">=</span><span class="s">{2021}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="zheng2019visdial-gcnn" class="col-sm-10"> <div class="title">Reasoning Visual Dialogs with Structural and Partial Observations <span class="abbr"> <abbr class="badge rounded">CVPR'19</abbr> </span> <span class="abbr"> <abbr class="badge rounded award">Oral</abbr> </span> </div> <div class="author"> <span style="font-weight:600">Zilong Zheng<sup>*</sup></span>, <a href="https://sites.google.com/view/wenguanwang/" rel="external nofollow noopener" target="_blank">Wenguan Wang<sup>*</sup></a>, <a href="https://web.cs.ucla.edu/~syqi/" rel="external nofollow noopener" target="_blank">Siyuan Qi<sup>*</sup></a>, and <a href="http://www.stat.ucla.edu/~sczhu/" rel="external nofollow noopener" target="_blank">Song-Chun Zhu</a><span class="periodical">, <em>in CVPR</em>, 2019. </span> <div class="periodical"> </div> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1904.05548" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/zilongzheng/visdial-gnn" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We propose a novel model to address the task of Visual Dialog which exhibits complex dialog structures. To obtain a reasonable answer based on the current question and the dialog history, the underlying semantic dependencies between dialog entities are essential. In this paper, we explicitly formalize this task as inference in a graphical model with partially observed nodes and unknown graph structures (relations in dialog). The given dialog entities are viewed as the observed nodes. The answer to a given question is represented by a node with missing value. We first introduce an Expectation Maximization algorithm to infer both the underlying dialog structures and the missing node values (desired answers). Based on this, we proceed to propose a differentiable graph neural network (GNN) solution that approximates this process. Experiment results on the VisDial and VisDial-Q datasets show that our model outperforms comparative methods. It is also observed that our method can infer the underlying dialog structure for better dialog reasoning. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zheng2019reasoning</span><span class="p">,</span>
    <span class="na">title</span><span class="p">=</span><span class="s">{Reasoning Visual Dialogs with Structural and Partial Observations}</span><span class="p">,</span>
    <span class="na">author</span><span class="p">=</span><span class="s">{Zheng, Zilong and Wang, Wenguan and Qi, Siyuan and Zhu, Song-Chun}</span><span class="p">,</span>
    <span class="na">booktitle</span><span class="p">=</span><span class="s">{Computer Vision and Pattern Recognition (CVPR), 2019 IEEE Conference on}</span><span class="p">,</span>
    <span class="na">year</span><span class="p">=</span><span class="s">{2019}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="3DDescriptorNet" class="col-sm-10"> <div class="title">Learning Descriptor Networks for 3D Shape Synthesis and Analysis <span class="abbr"> <abbr class="badge rounded">CVPR'18</abbr> </span> <span class="abbr"> <abbr class="badge rounded award">Oral</abbr> </span> </div> <div class="author"> <a href="http://www.stat.ucla.edu/~jxie/" rel="external nofollow noopener" target="_blank">Jianwen Xie<sup>*</sup></a>, <span style="font-weight:600">Zilong Zheng<sup>*</sup></span>, <a href="http://www.stat.ucla.edu/~ruiqigao/" rel="external nofollow noopener" target="_blank">Ruiqi Gao</a>, <a href="https://sites.google.com/view/wenguanwang/" rel="external nofollow noopener" target="_blank">Wenguan Wang</a>, <a href="http://www.stat.ucla.edu/~sczhu/" rel="external nofollow noopener" target="_blank">Song-Chun Zhu</a>, and <a href="http://www.stat.ucla.edu/~ywu/research.html" rel="external nofollow noopener" target="_blank">Ying Nian Wu</a><span class="periodical">, <em>in CVPR</em>, 2018. </span> <div class="periodical"> </div> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1804.00586" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/jianwen-xie/3DDescriptorNet" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="http://www.stat.ucla.edu/~jxie/3DDescriptorNet/3DDescriptorNet.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>This paper proposes a 3D shape descriptor network, which is a deep convolutional energy-based model, for modeling volumetric shape patterns. The maximum likelihood training of the model follows an “analysis by synthesis” scheme and can be interpreted as a mode seeking and mode shifting process. The model can synthesize 3D shape patterns by sampling from the probability distribution via MCMC such as Langevin dynamics. The model can be used to train a 3D generator network via MCMC teaching. The conditional version of the 3D shape descriptor net can be used for 3D object recovery and 3D object super-resolution. Experiments demonstrate that the proposed model can generate realistic 3D shape patterns and can be useful for 3D shape analysis. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">xie2018learning</span><span class="p">,</span>
    <span class="na">title</span><span class="p">=</span><span class="s">{Learning Descriptor Networks for 3D Shape Synthesis and Analysis}</span><span class="p">,</span>
    <span class="na">author</span><span class="p">=</span><span class="s">{Xie, Jianwen and Zheng, Zilong and Gao, Ruiqi and Wang, Wenguan and Zhu, Song-Chun and Wu, Ying Nian}</span><span class="p">,</span>
    <span class="na">booktitle</span><span class="p">=</span><span class="s">{Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)}</span><span class="p">,</span>
    <span class="na">pages</span><span class="p">=</span><span class="s">{8629--8638}</span><span class="p">,</span>
    <span class="na">year</span><span class="p">=</span><span class="s">{2018}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Zilong Zheng. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: June 13, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-2827DVQB1G"></script> <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'G-2827DVQB1G');
  </script> <script defer src="/assets/js/google-analytics-setup.js"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?eaf77346e117baa09987a278a117b9a7"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?f8abf2f636f242d077f24149a0a56c96"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>